<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Khaled Abughoush</title>

    <meta name="author" content="Khaled Abughoush">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Khaled Abughoush
                </p>
                <p>I'm a data scientist at <a href="https://www.pwc.com/us/en/industries/financial-services/financial-crimes.html">Financial Crimes</a> in New York City, where I advice clients on analytics and strategies to combat <a href="https://www.pwc.com/us/en/industries/financial-services/financial-crimes/fraud.html">Fraud</a>.
                </p>
                <p>
                   I did my Masters at <a href="https://www.orie.cornell.edu/orie">Cornell</a>, where I took courses under <a href="https://tech.cornell.edu/people/huseyin-topaloglu/">Huseyin Topaloglu</a>, <a href="https://tech.cornell.edu/people/andrea-lodi/">Andrea Lodi</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:KA363@CORNELL.EDU">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Pk5pQMgAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/ellord_kosh">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/kabughoush/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Profile.JPG"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Profile.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in fraud analytics, compliance, generative AI, and image processing. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="images/nuvo.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nuvo.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://pratulsrinivasan.github.io/nuvo/">
          <span class="papertitle">Unveiling the Dynamics of Open-Source AI Models: Development Trends, Industry Applications, and Challenges</span>
        </a>
        <br>
        <a href="https://www.usfca.edu/management/faculty/mana-azarm/">Mana Azarm</a>,
        <a href="https://www.usfca.edu/faculty/esmat-yasi-sangari/">Esmat Sangari</a>,
		<strong>Khaled Abughoush</strong>
        <br>
        <em>HICSS</em>, 2025
        <br>
        <a href="https://scholarspace.manoa.hawaii.edu/items/76d0769d-9154-42a8-bcd2-94a95b7b0437">arXiv</a>
        <p></p>
        <p>
        Open-source AI model development has seen exponential growth since 2020, with text processing dominating the field while audio and image processing lag behind, and just 1% of models, primarily in text processing, account for the majority of user engagement
        </p>
      </td>
    </tr>


    <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cat3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cat3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function cat3d_start() {
            document.getElementById('cat3d_image').style.opacity = "1";
          }

          function cat3d_stop() {
            document.getElementById('cat3d_image').style.opacity = "0";
          }
          cat3d_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://cat3d.github.io/">
			<span class="papertitle">CAT3D: Create Anything in 3D with Multi-View Diffusion Models
</span>
        </a>
        <br>
				<a href="https://ruiqigao.github.io/">Ruiqi Gao</a>*,
        <a href="https://holynski.org/">Aleksander Holynski</a>*, 
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>, 
				<a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
				<strong>Jonathan T. Barron</strong>,
        <a href="https://poolio.github.io/">Ben Poole</a>*

        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://cat3d.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
        <p></p>
        <p>
				A single model built around diffusion and NeRF that does text-to-3D, image-to-3D, and few-view reconstruction, trains in 1 minute, and renders at 60FPS in a browser.
        </p>
      </td>
    </tr>


    <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bog_image'><video  width=100% muted autoplay loop>
          <source src="images/bog.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/bog.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://creiser.github.io/binary_opacity_grid/">
          <span class="papertitle">Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis
</span>
        </a>
        <br>
				<a href="https://creiser.github.io/">Christian Reiser</a>,
				<a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
				<a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
				<a href="https://dorverbin.github.io/">Dor Verbin</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<strong>Jonathan T. Barron</strong>,
				<a href="https://phogzone.com/">Peter Hedman</a>*,
				<a href="https://www.cvlibs.net/">Andreas Geiger</a>*		
        <br>
        <em>SIGGRAPH</em>, 2024
        <br>
        <a href="https://creiser.github.io/binary_opacity_grid/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=2TPUmGRg8bM">video</a>
        /
        <a href="https://arxiv.org/abs/2402.12377">arXiv</a>
        <p></p>
        <p>
        Applying anti-aliasing to a discrete opacity grid lets you render a hard representation into a soft image, and this enables highly-detailed mesh recovery.
        </p>
      </td>
    </tr>

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/smerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/smerf.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://smerf-3d.github.io/">
          <span class="papertitle">SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration</span>
        </a>
        <br>
		<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth*</a>,
		<a href="https://phogzone.com/">Peter Hedman*</a>,
		<a href="https://creiser.github.io/">Christian Reiser</a>,
		<a href="">Peter Zhizhin</a>,
		<a href="">Jean-François Thibert</a>,
        <a href="https://lucic.ai/">Mario Lučić</a>,
        <a href="https://szeliski.org/">Richard Szeliski</a>,
		<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH</em>, 2024 &nbsp <font color="red"><strong>(Honorable Mention)</strong></font>
        <br>
        <a href="https://smerf-3d.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
      </td>
    </tr>
	


    <tr onmouseout="internerf_stop()" onmouseover="internerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='internerf_image'>
					  <img src='images/internerf_after.jpg' width=100%>
					</div>
          <img src='images/internerf_before.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function internerf_start() {
            document.getElementById('internerf_image').style.opacity = "1";
          }

          function internerf_stop() {
            document.getElementById('internerf_image').style.opacity = "0";
          }
          internerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2406.11737">
          <span class="papertitle">InterNeRF: Scaling Radiance Fields via Parameter Interpolation</span>
        </a>
        <br>
		<a href="https://clintonjwang.github.io/">Clinton Wang</a>,
		<a href="https://phogzone.com/">Peter Hedman</a>,
		<a href="https://people.csail.mit.edu/polina/">Polina Golland</a>,
		<strong>Jonathan T. Barron</strong>,
		<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth</a>
        <br>
        <em>CVPR Neural Rendering Intelligence</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2406.11737">arXiv</a>
        <p></p>
        <p>
        Parameter interpolation enables high-quality large-scale scene reconstruction and out-of-core training and rendering.
        </p>
      </td>
    </tr>

  <tr onmouseout="eclipse_stop()" onmouseover="eclipse_start()">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='eclipse_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/eclipse_after.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/eclipse_before.jpg' width="160">
      </div>
      <script type="text/javascript">
        function eclipse_start() {
          document.getElementById('eclipse_image').style.opacity = "1";
        }

        function eclipse_stop() {
          document.getElementById('eclipse_image').style.opacity = "0";
        }
        eclipse_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://dorverbin.github.io/eclipse">
        <span class="papertitle">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</span>
      </a>
      <br>
      <a href="https://dorverbin.github.io/">Dor Verbin</a>,
      <a href="https://bmild.github.io/">Ben Mildenhall</a>,
      <a href="https://phogzone.com/">Peter Hedman</a>, <br>
      <strong>Jonathan T. Barron</strong>,
      <a href="Todd Zickler">Todd Zickler</a>,
      <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
      <br>
      <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
      <br>
      <a href="https://dorverbin.github.io/eclipse">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=amQLGyza3EU">video</a>
      /
      <a href="https://arxiv.org/abs/2305.16321">arXiv</a>
      <p></p>
      <p>
      Shadows cast by unobserved occluders provide a high-frequency cue for recovering illumination and materials.
      </p>
    </td>
  </tr>
	

  <tr onmouseout="shinobi_stop()" onmouseover="shinobi_start()">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='shinobi_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/shinobi.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/shinobi.jpg' width="160">
      </div>
      <script type="text/javascript">
        function shinobi_start() {
          document.getElementById('shinobi_image').style.opacity = "1";
        }

        function shinobi_stop() {
          document.getElementById('shinobi_image').style.opacity = "0";
        }
        shinobi_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://shinobi.aengelhardt.com/">
        <span class="papertitle">SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-Wild</span>
      </a>
      <br>
			
			<a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/">Andreas Engelhardt</a>, 
			<a href="https://amitraj93.github.io/">Amit Raj</a>, 
			<a href="https://markboss.me/">Mark Boss</a>, 
			<a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>, 
			<a href="https://abhishekkar.info/">Abhishek Kar</a>, 
			<a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
			<a href="https://deqings.github.io/">Deqing Sun</a>, 
			<a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
      <strong>Jonathan T. Barron</strong>,
			<a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/">Hendrik P.A. Lensch</a>, 
			<a href="https://varunjampani.github.io/">Varun Jampani</a>
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://shinobi.aengelhardt.com/">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=m_5kvtlDnl4">video</a>
      /
      <a href="https://arxiv.org/abs/2401.10171">arXiv</a>
      <p></p>
      <p>
      A class-agnostic inverse rendering solution for turning in-the-wild images of an object into a relightable 3D asset.
      </p>
    </td>
  </tr>
	
	
    <tr onmouseout="recon_stop()" onmouseover="recon_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/recon.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/recon.png' width="160">
        </div>
        <script type="text/javascript">
          function recon_start() {
            document.getElementById('recon_image').style.opacity = "1";
          }

          function recon_stop() {
            document.getElementById('recon_image').style.opacity = "0";
          }
          recon_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://reconfusion.github.io/">
			<span class="papertitle">ReconFusion: 3D Reconstruction with Diffusion Priors</span>
        </a>
        <br>
        <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu*</a>,
		<a href="https://bmild.github.io/">Ben Mildenhall*</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
        <a href="https://scholar.google.com/citations?user=_pKKv2QAAAAJ&hl=en/">Daniel Watson</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
		<strong>Jonathan T. Barron</strong>,
        <a href="https://poolio.github.io/">Ben Poole</a>,
        <a href="https://holynski.org/">Aleksander Holynski*</a>
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="https://reconfusion.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2312.02981">arXiv</a>
        <p></p>
        <p>
        Using a multi-image diffusion model as a regularizer lets you recover high-quality radiance fields from just a handful of images.
        </p>
      </td>
    </tr>


<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='difsurvey_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/difsurvey_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video></div>
      <img src='images/difsurvey_image.jpg' width="160">
    </div>
    <script type="text/javascript">
      function difsurvey_start() {
        document.getElementById('difsurvey_image').style.opacity = "1";
      }

      function difsurvey_stop() {
        document.getElementById('difsurvey_image').style.opacity = "0";
      }
      difsurvey_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2310.07204">
      <span class="papertitle">State of the Art on Diffusion Models for Visual Computing
</span>
    </a>
    <br>
	<a href="https://ryanpo.com/">Ryan Po</a>,
	<a href="https://yifita.netlify.app/">Wang Yifan</a>,
	<a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>,
	<a href="https://kfiraberman.github.io/">Kfir Aberman</a>,
	<strong>Jonathan T. Barron</strong>,
	<a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
	<a href="https://ericryanchan.github.io/">Eric Ryan Chan</a>,
	<a href="https://www.weizmann.ac.il/math/dekel/home">Tali Dekel</a>,
	<a href="https://holynski.org/">Aleksander Holynski</a>,
	<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
	<a href="https://tml.stanford.edu/">C. Karen Liu</a>,
	<a href="https://lingjie0206.github.io/">Lingjie Liu</a>,
	<a href="https://bmild.github.io/">Ben Mildenhall</a>,
    <a href="https://www.niessnerlab.org/">Matthias Nießner</a>,
	<a href="https://ommer-lab.com/people/ommer/">Björn Ommer</a>,
	<a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>,
	<a href="https://peterwonka.net/">Peter Wonka</a>,
    <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
    <br>
	<em>Eurographics State-of-the-Art Report<em>, 2024
    <br>
    <p></p>
    <p>
    A survey of recent progress in diffusion models for images, videos, and 3D.
    </p>
  </td>
</tr>          

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/camp.png' width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://camp-nerf.github.io/">
          <span class="papertitle">CamP: Camera Preconditioning for Neural Radiance Fields</span>
        </a>
        <br>
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>
        <br>
        <em>SIGGRAPH Asia</em>, 2023
        <br>
        <a href="https://camp-nerf.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2308.10902">arXiv</a>
        <p></p>
        <p>
        Preconditioning based on camera parameterization helps NeRF and camera extrinsics/intrinsics optimize better together.
        </p>
      </td>
    </tr>

    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</span>
          </a>
          <br>
          <strong>Jonathan T. Barron</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
          <br>
          <a href="http://jonbarron.info/zipnerf">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
          /
          <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
          <p></p>
          <p>
          Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x.
          </p>
        </td>
      </tr>
      
      
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/owl.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dreambooth3d.github.io/">
            <span class="papertitle">DreamBooth3D: Subject-Driven Text-to-3D Generation</span>
          </a>
          <br>
          
  <a href="https://amitraj93.github.io/">Amit Raj</a>, <a href="https://www.linkedin.com/in/srinivas-kaza-64223b74">Srinivas Kaza</a>, <a href="https://poolio.github.io/">Ben Poole</a>, <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>, 
  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>, <a href="https://kfiraberman.github.io/">Kfir Aberman</a>, <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, 
          <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://dreambooth3d.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2303.13508">arXiv</a>
          <p></p>
          <p>Combining DreamBooth (personalized text-to-image) and DreamFusion (text-to-3D) yields high-quality, subject-specific 3D assets with text-driven modifications</p>
        </td>
      </tr>

      

      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://bakedsdf.github.io/">
            <span class="papertitle">BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</span>
          </a>
          <br>
          <a href="https://lioryariv.github.io/">Lior Yariv*</a>,
          <a href="https://phogzone.com/">Peter Hedman*</a>,
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,  <br>
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://bakedsdf.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a>
          /
          <a href="https://arxiv.org/abs/2302.14859">arXiv</a>
          <p></p>
          <p>
          We use SDFs to bake a NeRF-like model into a high quality mesh and do real-time view synthesis.
          </p>
        </td>
      </tr>


      <tr onmouseout="merf_stop()" onmouseover="merf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='merf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/merf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/merf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function merf_start() {
              document.getElementById('merf_image').style.opacity = "1";
            }

            function merf_stop() {
              document.getElementById('merf_image').style.opacity = "0";
            }
            merf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://merf42.github.io/">
            <span class="papertitle">MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes</span>
          </a>
          <br>
          <a href="https://creiser.github.io/">Christian Reiser</a>,
          <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
          <a href="https://dorverbin.github.io/">Dor Verbin</a>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
          <strong>Jonathan T. Barron</strong>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>SIGGRAPH</em>, 2023
          <br>
          <a href="https://merf42.github.io/">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=3EACM2JAcxc">video</a>
          /
          <a href="https://arxiv.org/abs/2302.12249">arXiv</a>
          <p></p>
          <p>
          We use volumetric rendering with a sparse 3D feature grid and 2D feature planes to do real-time view synthesis.
          </p>
        </td>
      </tr>



      <tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='alignerf_image'>
              <img src='images/alignerf_after.jpg' width="160"></div>
            <img src='images/alignerf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function alignerf_start() {
              document.getElementById('alignerf_image').style.opacity = "1";
            }

            function alignerf_stop() {
              document.getElementById('alignerf_image').style.opacity = "0";
            }
            alignerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://yifanjiang19.github.io/alignerf">
            <span class="papertitle">AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training</span>
          </a>
          <br>
          <a href="https://yifanjiang.net/">Yifan Jiang</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>, 
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://ir1d.github.io/">Dejia Xu</a>, <br>
          <strong>Jonathan T. Barron</strong>,
          <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
          <a href="https://tianfan.info/">Tianfan Xue</a>
          <br>
          <em>CVPR</em>, 2023
          <br>
          <a href="https://yifanjiang19.github.io/alignerf">project page</a>
          /
          <a href="https://arxiv.org/abs/2211.09682">arXiv</a>
          <p></p>
          <p>
          Accounting for misalignment due to scene motion or calibration errors improves NeRF reconstruction quality.
          </p>
        </td>
      </tr>
      
  <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/dreamfusion.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/dreamfusion.jpg' width="160">
      </div>
      <script type="text/javascript">
        function dreamfusion_start() {
          document.getElementById('dreamfusion_image').style.opacity = "1";
        }

        function dreamfusion_stop() {
          document.getElementById('dreamfusion_image').style.opacity = "0";
        }
        dreamfusion_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://dreamfusion3d.github.io/">
        <span class="papertitle">DreamFusion: Text-to-3D using 2D Diffusion</span>
      </a>
      <br>
      <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
      <a href="https://www.ajayj.com/">Ajay Jain</a>,
      <strong>Jonathan T. Barron</strong>,
      <a href="https://bmild.github.io/">Ben Mildenhall</a>
      <br>
      <em>ICLR</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Outstanding Paper Award)</strong></font>
      <br>
      <a href="https://dreamfusion3d.github.io/">project page</a>
      /
      <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
      /
      <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
      <p></p>
      <p>
      We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling.
      </p>
    </td>
  </tr>

  <tr onmouseout="guandao_stop()" onmouseover="guandao_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='guandao_image'>
        <img src='images/guandao_after.png' width="160"></div>
      <img src='images/guandao_before.png' width="160">
    </div>
    <script type="text/javascript">
      function guandao_start() {
        document.getElementById('guandao_image').style.opacity = "1";
      }

      function guandao_stop() {
        document.getElementById('guandao_image').style.opacity = "0";
      }
      guandao_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2304.14473">
      <span class="papertitle">Learning a Diffusion Prior for NeRFs</span>
    </a>
    <br>
    <a href="https://www.guandaoyang.com/">Guandao Yang</a>, 
    <a href="https://abhijitkundu.info/">Abhijit Kundu</a>, 
    <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a>, 
    <strong>Jonathan T. Barron</strong>, 
    <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
    <br>
    <em>ICLR Workshop</em>, 2023
    <p></p>
    <p>
      Training a diffusion model on grid-based NeRFs lets you (conditionally) sample NeRFs.
    </p>
  </td>
  </tr>

            <tr onmouseout="mira_stop()" onmouseover="mira_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mira_image'>
                    <img src='images/mira_after.jpg' width="160"></div>
                  <img src='images/mira_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function mira_start() {
                    document.getElementById('mira_image').style.opacity = "1";
                  }

                  function mira_stop() {
                    document.getElementById('mira_image').style.opacity = "0";
                  }
                  mira_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=AmPeAFzU3a4">
                  <span class="papertitle">MIRA: Mental Imagery for Robotic Affordances</span>
                </a>
                <br>
                <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
                <a href="http://www.peteflorence.com/">Pete Florence</a>, 
                <a href="https://andyzeng.github.io/">Andy Zeng</a>, <strong>Jonathan T. Barron</strong>, 
                <a href="https://yilundu.github.io/">Yilun Du</a>,
                <a href="https://people.csail.mit.edu/weichium/">Wei-Chiu Ma</a>,
                <a href="https://anthonysimeonov.github.io/">Anthony Simeonov</a>,
                <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
                <br>
                <em>CoRL</em>, 2022
                <p></p>
                <p>
                  NeRF lets us synthesize novel orthographic views that work well with pixel-wise algorithms for robotic manipulation.
                </p>
              </td>
            </tr>		
            
            <tr onmouseout="samurai_stop()" onmouseover="samurai_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='samurai_image'>
                    <img src='images/samurai_after.jpg' width="160"></div>
                  <img src='images/samurai_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function samurai_start() {
                    document.getElementById('samurai_image').style.opacity = "1";
                  }

                  function samurai_stop() {
                    document.getElementById('samurai_image').style.opacity = "0";
                  }
                  samurai_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://markboss.me/publication/2022-samurai/">
                  <span class="papertitle">SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image Collections</span>
                </a>
                <br>
                <a href="https://markboss.me">Mark Boss</a>, 
                <a href="">Andreas Engelhardt</a>, 
                <a href="https://abhishekkar.info/">Abhishek Kar</a>, 
                <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
                <a href="https://deqings.github.io/">Deqing Sun</a>, 
                <strong>Jonathan T. Barron</strong>,
                <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>,
                <a href="https://varunjampani.github.io">Varun Jampani</a>
                <br>
                <em>NeurIPS</em>, 2022
                <br>
                <a href="https://markboss.me/publication/2022-samurai/">project page</a> /
                <a href="https://www.youtube.com/watch?v=LlYuGDjXp-8">video</a> /
                <a href="https://arxiv.org/abs/2205.15768">arXiv</a>
                <p></p>
                <p>
  A joint optimization framework for estimating shape, BRDF, camera pose, and illumination from in-the-wild image collections.
                </p>
              </td>
            </tr>		

            <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
            <div class="two" id='pnf_image'>
              <img src='images/pnf_before.jpg' width="160"></div>
            <img src='images/pnf_after.jpg' width="160">
            </div>
            <script type="text/javascript">
            function pnf_start() {
              document.getElementById('pnf_image').style.opacity = "1";
            }

            function pnf_stop() {
              document.getElementById('pnf_image').style.opacity = "0";
            }
            pnf_stop()
            </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="TODO">
            <span class="papertitle">Polynomial Neural Fields for Subband Decomposition</span>
            </a> <br>
            <a href="https://www.guandaoyang.com/">Guandao Yang*</a>,
            <a href="https://sagiebenaim.github.io/">Sagie Benaim*</a>,
            <a href="https://varunjampani.github.io/">Varun Jampani</a>,
            <a href="https://www.kylegenova.com/">Kyle Genova</a>,
            <strong>Jonathan T. Barron</strong>,
            <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
            <a href="http://home.bharathh.info/">Bharath Hariharan</a>,
            <a href="https://sergebelongie.github.io/">Serge Belongie</a>
            <br>
            <em>NeurIPS</em>, 2022
            <p>
            Representing neural fields as a composition of manipulable and interpretable components lets you do things like reason about frequencies and scale.
            </p>
            </td>
            </tr> 


            <tr onmouseout="malle_stop()" onmouseover="malle_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='malle_image'>
                    <img src='images/MalleConv_after.jpg' width="160"></div>
                  <img src='images/MalleConv_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function malle_start() {
                    document.getElementById('malle_image').style.opacity = "1";
                  }

                  function malle_stop() {
                    document.getElementById('malle_image').style.opacity = "0";
                  }
                  malle_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://yifanjiang.net/MalleConv.html">
                  <span class="papertitle">Fast and High-Quality Image Denoising via Malleable Convolutions</span>
                </a>
                <br>
                <a href="https://yifanjiang.net/">Yifan Jiang</a>,
                <a href="https://bartwronski.com/">Bartlomiej Wronski</a>, 
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>
                <br>
                <em>ECCV</em>, 2022
                <br>
                <a href="https://yifanjiang.net/MalleConv.html">project page</a>
                /
                <a href="https://arxiv.org/abs/2201.00392">arXiv</a>
                <p></p>
                <p>
                We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerf_supervision.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerf_supervision.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('nerfsuper_image').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('nerfsuper_image').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://yenchenlin.me/nerf-supervision/">
                  <span class="papertitle">NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</span>
                </a>
                <br>
                <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
                <a href="http://www.peteflorence.com/">Pete Florence</a>, 
                <strong>Jonathan T. Barron</strong>,  <br>
                <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
                <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
                <br>
                <em>ICRA</em>, 2022  
                <br>
                <a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
                <a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
                <a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
                <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
                <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
                <p></p>
                <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>
              </td>
            </tr>

            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/refnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/refnerf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function refnerf_start() {
                    document.getElementById('refnerf_image').style.opacity = "1";
                  }

                  function refnerf_stop() {
                    document.getElementById('refnerf_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dorverbin.github.io/refnerf/index.html">
                    <span class="papertitle">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</span>
                  </a>
                  <br>
                  <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                  <a href="Todd Zickler">Todd Zickler</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
            <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
                  <br>
                  <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
            /
                  <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
            /
                  <a href="https://youtu.be/qrdRH9irAlk">video</a>
                  <p></p>
                  <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>
                </td>
              </tr>
              
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/mip360_sat.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/mip360_sat.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function mip360_start() {
                    document.getElementById('mip360_image').style.opacity = "1";
                  }

                  function mip360_stop() {
                    document.getElementById('mip360_image').style.opacity = "0";
                  }
                  mip360_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://jonbarron.info/mipnerf360">
                  <span class="papertitle">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://phogzone.com/">Peter Hedman</a>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="http://jonbarron.info/mipnerf360">project page</a>
                /
                <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
                /
                <a href="https://youtu.be/zBSH-k9GbV4">video</a>
                <p></p>
                <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>
              </td>
            </tr> 

            <tr onmouseout="rawnerf_stop()" onmouseover="rawnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='rawnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/rawnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/rawnerf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function rawnerf_start() {
                    document.getElementById('rawnerf_image').style.opacity = "1";
                  }

                  function rawnerf_stop() {
                    document.getElementById('rawnerf_image').style.opacity = "0";
                  }
                  rawnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://bmild.github.io/rawnerf/index.html">
                  <span class="papertitle">NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</span>
                </a>
                <br>
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://bmild.github.io/rawnerf/index.html">project page</a>
          /
                <a href="https://arxiv.org/abs/2111.13679">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=JtBS4KBcKVc">video</a>
                <p></p>
                <p>
                  Properly training NeRF on raw camera data enables HDR view synthesis and bokeh, and outperforms multi-image denoising.</p>
              </td>
            </tr> 
            
    
            <tr onmouseout="regnerf_stop()" onmouseover="regnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='regnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/regnerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/regnerf_before.jpeg' width="160">
                </div>
                <script type="text/javascript">
                  function regnerf_start() {
                    document.getElementById('regnerf_image').style.opacity = "1";
                  }

                  function regnerf_stop() {
                    document.getElementById('regnerf_image').style.opacity = "0";
                  }
                  regnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/regnerf/index.html">
                  <span class="papertitle">RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</span>
                </a>
                <br>
                <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                <a href="https://msmsajjadi.github.io/">Mehdi S. M. Sajjadi</a>, 
                <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
                <a href="http://www2.informatik.uni-freiburg.de/~radwann/">Noha Radwan</a>
                <br>
          <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://m-niemeyer.github.io/regnerf/index.html">project page</a>
          /
                <a href="https://arxiv.org/abs/2112.00724">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=QyyyvA4-Kwc">video</a>
                <p></p>
                <p>Regularizing unseen views during optimization enables view synthesis from as few as 3 input images.</p>
              </td>
            </tr> 


            <tr onmouseout="blocknerf_stop()" onmouseover="blocknerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='blocknerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/blocknerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/blocknerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function blocknerf_start() {
                    document.getElementById('blocknerf_image').style.opacity = "1";
                  }

                  function blocknerf_stop() {
                    document.getElementById('blocknerf_image').style.opacity = "0";
                  }
                  blocknerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://waymo.com/research/block-nerf/">
                  <span class="papertitle">Block-NeRF: Scalable Large Scene Neural View Synthesis</span>
                </a>
                <br>
                <a href="http://matthewtancik.com/">Matthew Tancik</a>,
                <a href="http://casser.io/">Vincent Casser</a>,
                <a href="https://sites.google.com/site/skywalkeryxc/">Xinchen Yan</a>,
                <a href="https://scholar.google.com/citations?user=5mJUkI4AAAAJ&hl=en">Sabeek Pradhan</a>, <br>
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.henrikkretzschmar.com/">Henrik Kretzschmar</a>
                <br>
          <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://waymo.com/research/block-nerf/">project page</a>
          /
                <a href="https://arxiv.org/abs/2202.05263">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=6lGMCAzBzOQ">video</a>
                <p></p>
                <p>We can do city-scale reconstruction by training multiple NeRFs with millions of images.</p>
              </td>
            </tr>
            
            <tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='hnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/hnerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/hnerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function hnerf_start() {
                    document.getElementById('hnerf_image').style.opacity = "1";
                  }

                  function hnerf_stop() {
                    document.getElementById('hnerf_image').style.opacity = "0";
                  }
                  hnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://grail.cs.washington.edu/projects/humannerf/">
                  <span class="papertitle">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</span>
                </a>
                <br>
                <a href="https://homes.cs.washington.edu/~chungyi/">Chung-Yi Weng</a>,
                <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman </a>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://grail.cs.washington.edu/projects/humannerf/">project page</a>
                /
                <a href="https://arxiv.org/abs/2201.04127">arXiv</a>
                /
                <a href="https://youtu.be/GM-RoZEymmw">video</a>
                <p></p>
                <p>Combining NeRF with pose estimation lets you use a monocular video to do free-viewpoint rendering of a human.</p>
              </td>
            </tr>
            
            <tr onmouseout="urf_stop()" onmouseover="urf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='urf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/urf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/urf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function urf_start() {
                    document.getElementById('urf_image').style.opacity = "1";
                  }

                  function urf_stop() {
                    document.getElementById('urf_image').style.opacity = "0";
                  }
                  urf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://urban-radiance-fields.github.io/">
                  <span class="papertitle">Urban Radiance Fields</span>
                </a>
                <br>
                <a href="http://www.krematas.com/">Konstantinos Rematas</a>,
                <a href="https://andrewhliu.github.io/">Andrew Liu</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
                <a href="https://www.cs.princeton.edu/~funk/">Tom Funkhouser</a>,
                <a href="https://sites.google.com/corp/view/vittoferrari"> Vittorio Ferrari</a>
                <br>
                <em>CVPR</em>, 2022
                <br>
                <a href="https://urban-radiance-fields.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2111.14643">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=qGlq5DZT6uc">video</a>
                <p></p>
                <p>
                  Incorporating lidar and explicitly modeling the sky lets you reconstruct urban environments.</p>
              </td>
            </tr> 

    
            </tr>

            <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='hdrnet_image'><img src='images/hdrnet_after.jpg'></div>
                  <img src='images/hdrnet_before.jpg'>
                </div>
                <script type="text/javascript">
                  function hdrnet_start() {
                    document.getElementById('hdrnet_image').style.opacity = "1";
                  }

                  function hdrnet_stop() {
                    document.getElementById('hdrnet_image').style.opacity = "0";
                  }
                  hdrnet_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1jQY3CTMnLX7PeGUzYLso9H1eCsZyWbwg/view?usp=sharing">
                  <span class="papertitle">Deep Bilateral Learning for Real-Time Image Enhancement</span>
                </a>
                <br>
                <a href="http://www.mgharbi.com">Micha&euml;l Gharbi</a>, <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>, <strong>Jonathan T. Barron</strong>, <a href="https://people.csail.mit.edu/hasinoff/">Samuel W. Hasinoff</a>, <a href="http://people.csail.mit.edu/fredo/">Fr&eacute;do Durand </a>
                <br>
                <em>SIGGRAPH</em>, 2017
                <br>
                <a href="https://groups.csail.mit.edu/graphics/hdrnet/">project page</a> /
                <a href="https://www.youtube.com/watch?v=GAe0qKKQY_I">video</a> /
                <a href="data/GharbiSIGGRAPH2017.bib">bibtex</a> /
                <a href="http://news.mit.edu/2017/automatic-image-retouching-phone-0802">p</a><a href="https://www.wired.com/story/googles-new-algorithm-perfects-photos-before-you-even-take-them/">r</a><a href="https://petapixel.com/2017/08/02/new-ai-can-retouch-photos-snap/">e</a><a href="https://www.theverge.com/2017/8/2/16082272/google-mit-retouch-photos-machine-learning">s</a><a href="http://gizmodo.com/clever-camera-app-uses-deep-learning-to-perfectly-retou-1797474282">s</a>
                <p></p>
                <p>By training a deep network in bilateral space we can learn a model for high-resolution and real-time image enhancement.</p>
              </td>
            </tr>

            <tr onmouseout="ffcc_stop()" onmouseover="ffcc_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='ffcc_image'><img src='images/ffcc_after.jpg'></div>
                  <img src='images/ffcc_before.jpg'>
                </div>
                <script type="text/javascript">
                  function ffcc_start() {
                    document.getElementById('ffcc_image').style.opacity = "1";
                  }

                  function ffcc_stop() {
                    document.getElementById('ffcc_image').style.opacity = "0";
                  }
                  ffcc_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1611.07596">
                  <span class="papertitle">Fast Fourier Color Constancy</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a>,
                <br>
                <em>CVPR</em>, 2017
                <br>
                <a href="https://youtu.be/rZCXSfl13rY">video</a> /
                <a href="data/BarronTsaiCVPR2017.bib">bibtex</a> /
                <a href="https://github.com/google/ffcc">code</a> /
                <a href="https://drive.google.com/open?id=0B4nuwEMaEsnmWkJQMlFPSFNzbEk">output</a> /
                <a href="https://blog.google/products/photos/six-tips-make-your-photos-pop/">blog post</a> /
                <a href="https://9to5google.com/2017/03/03/google-photos-auto-white-balance/">p</a><a href="https://www.engadget.com/2017/03/03/google-photos-automatically-fixes-your-pictures-white-balance/">r</a><a href="https://lifehacker.com/google-photos-will-now-automatically-adjust-the-white-b-1793009155">e</a><a href="https://petapixel.com/2017/03/06/google-photos-will-now-automatically-white-balance-snapshots/">s</a><a href="http://www.theverge.com/2017/3/3/14800062/google-photos-auto-white-balance-android">s</a>
                <p></p>
                <p>Color space can be aliased, allowing white balance models to be learned and evaluated in the frequency domain. This improves accuracy by 13-20% and speed by 250-3000x.</p>
                <p>This technology is used by <a href="https://store.google.com/product/pixel_compare">Google Pixel</a>, <a href="https://photos.google.com/">Google Photos</a>, and <a href="https://www.google.com/maps">Google Maps</a>.</p>
              </td>
            </tr>

            <tr onmouseout="jump_stop()" onmouseover="jump_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='jump_image'><img src='images/jump_anim.gif'></div>
                  <img src='images/jump_still.png'>
                </div>
                <script type="text/javascript">
                  function jump_start() {
                    document.getElementById('jump_image').style.opacity = "1";
                  }

                  function jump_stop() {
                    document.getElementById('jump_image').style.opacity = "0";
                  }
                  jump_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1RBnTrtzqmuO8uj3GQaR5vBJZjIC3Jxjn/view?usp=sharing">
                  <span class="papertitle">Jump: Virtual Reality Video</span>
                </a>
                <br>
                <a href="http://mi.eng.cam.ac.uk/~ra312/">Robert Anderson</a>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <strong>Jonathan T. Barron</strong>, <a href="https://mediatech.aalto.fi/~janne/index.php">Janne Kontkanen</a>, <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, <a href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>, <a href="https://homes.cs.washington.edu/~sagarwal/">Sameer Agarwal</a>, <a href="https://homes.cs.washington.edu/~seitz/">Steven M Seitz</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2016
                <br>
                <a href="https://drive.google.com/file/d/11D4eCDXqqFTtZT0WS2COJE0hsAN3QEww/view?usp=sharing">supplement</a> /
                <a href="https://www.youtube.com/watch?v=O0qUYynupTI">video</a> /
                <a href="data/Anderson2016.bib">bibtex</a> /
                <a href="https://blog.google/products/google-vr/jump-using-omnidirectional-stereo-vr-video/">blog post</a>
                <p></p>
                <p>Using computer vision and a ring of cameras, we can make video for virtual reality headsets that is both stereo and 360&deg;.</p>
                <p>This technology is used by <a href="https://vr.google.com/jump/">Jump</a>. </p>
              </td>
            </tr>

            <tr onmouseout="hdrp_stop()" onmouseover="hdrp_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='hdrp_image'><img src='images/hdrp_after.jpg'></div>
                  <img src='images/hdrp_before.jpg'>
                </div>
                <script type="text/javascript">
                  function hdrp_start() {
                    document.getElementById('hdrp_image').style.opacity = "1";
                  }

                  function hdrp_stop() {
                    document.getElementById('hdrp_image').style.opacity = "0";
                  }
                  hdrp_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1SSSmVHWbMQ7sZMOredSVWVJXbXobkyzA/view?usp=sharing">
                  <span class="papertitle">Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras</span>
                </a>
                <br>
                <a href="http://people.csail.mit.edu/hasinoff/">Samuel W. Hasinoff</a>, <a href="http://www.dsharlet.com/">Dillon Sharlet</a>, <a href="http://www.geisswerks.com/">Ryan Geiss</a>, <a href="http://people.csail.mit.edu/abadams/">Andrew Adams</a>, <strong>Jonathan T. Barron</strong>, Florian Kainz, <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>, <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2016
                <br>
                <a href="http://hdrplusdata.org/">project page</a> /
                <a href="https://drive.google.com/open?id=15EUuSDi1BtHUgQCaiooVrD44qYKIC3vx">supplement</a> /
                <a href="data/Hasinoff2016.bib">bibtex</a>
                <p></p>
                <p>Mobile phones can take beautiful photographs in low-light or high dynamic range environments by aligning and merging a burst of images.</p>
                <p>This technology is used by the <a href="https://research.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">Nexus HDR+</a> feature.</p>
              </td>
            </tr>

            <tr onmouseout="bs_stop()" onmouseover="bs_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='bs_image'><img src='images/BS_after.jpg'></div>
                  <img src='images/BS_before.jpg'>
                </div>
                <script type="text/javascript">
                  function bs_start() {
                    document.getElementById('bs_image').style.opacity = "1";
                  }

                  function bs_stop() {
                    document.getElementById('bs_image').style.opacity = "0";
                  }
                  bs_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1zFzCaFwkGK1EGmJ_KEqb-ZsRJhfUKN2S/view?usp=sharing">
                  <span class="papertitle">The Fast Bilateral Solver</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
                <br>
                <em>ECCV</em>, 2016 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
                <br>
                <a href="http://arxiv.org/abs/1511.03296">arXiv</a> /
                <a href="data/BarronPooleECCV2016.bib">bibtex</a> /
                <a href="http://videolectures.net/eccv2016_barron_bilateral_solver/">video (they messed up my slides, use &rarr;)</a> /
                <a href="https://drive.google.com/file/d/19x1AeN0PFus6Pjrd8nR-vCmJ6bNEefsC/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/file/d/1p9nduiymK9jUh7WfwlsMjBfW8RoNe_61/view?usp=sharing">PDF</a>) /
                <a href="https://github.com/poolio/bilateral_solver">code</a> /
                <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmaDI3bm5VeDRxams/view?usp=sharing&resourcekey=0-pmkbnOuy8caA7-3GGSfeNQ">depth super-res results</a> /
                <a href="data/BarronPooleECCV2016_reviews.txt">reviews</a>
                <p></p>
                <p>Our solver smooths things better than other filters and faster than other optimization algorithms, and you can backprop through it.</p>
              </td>
            </tr>

            <tr onmouseout="diverdi_stop()" onmouseover="diverdi_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='diverdi_image'><img src='images/diverdi_after.jpg'></div>
                  <img src='images/diverdi_before.jpg'>
                </div>
                <script type="text/javascript">
                  function diverdi_start() {
                    document.getElementById('diverdi_image').style.opacity = "1";
                  }

                  function diverdi_stop() {
                    document.getElementById('diverdi_image').style.opacity = "0";
                  }
                  diverdi_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1mmT-LuK_eBZsl3qp4-fAshEPdgfbgvNE/view?usp=sharing">
                  <span class="papertitle">Geometric Calibration for Mobile, Stereo, Autofocus Cameras</span>
                </a>
                <br>
                <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>WACV</em>, 2016
                <br>
                <a href="data/Diverdi2016.bib">bibtex</a>
                <p></p>
                <p>Standard techniques for stereo calibration don't work for cheap mobile cameras.</p>
              </td>
            </tr>

            <tr onmouseout="dt_stop()" onmouseover="dt_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dt_image'><img src='images/DT_edge.jpg'></div>
                  <img src='images/DT_image.jpg'>
                </div>
                <script type="text/javascript">
                  function dt_start() {
                    document.getElementById('dt_image').style.opacity = "1";
                  }

                  function dt_stop() {
                    document.getElementById('dt_image').style.opacity = "0";
                  }
                  dt_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/178Xj2PZ1w6hZJpucU-TiZOoCemJmvsVQ/view?usp=sharing">
                  <span class="papertitle">Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform</span>
                </a>
                <br>
                <em>CVPR</em>, 2016
                <br>
                <a href="http://liangchiehchen.com/">Liang-Chieh Chen</a>, <strong>Jonathan T. Barron</strong>, <a href="http://ttic.uchicago.edu/~gpapan/">George Papandreou</a>, <a href="http://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>, <a href="http://www.stat.ucla.edu/~yuille/">Alan L. Yuille</a>
                <br>
                <a href="data/Chen2016.bib">bibtex</a> /
                <a href="http://liangchiehchen.com/projects/DeepLab.html">project page</a> /
                <a href="https://bitbucket.org/aquariusjay/deeplab-public-ver2">code</a>
                <p></p>
                <p>By integrating an edge-aware filter into a convolutional neural network we can learn an edge-detector while improving semantic segmentation.</p>
              </td>
            </tr>

            <tr onmouseout="ccc_stop()" onmouseover="ccc_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='ccc_image'><img src='images/ccc_after.jpg'></div>
                  <img src='images/ccc_before.jpg'>
                </div>
                <script type="text/javascript">
                  function ccc_start() {
                    document.getElementById('ccc_image').style.opacity = "1";
                  }

                  function ccc_stop() {
                    document.getElementById('ccc_image').style.opacity = "0";
                  }
                  ccc_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1id74VNDL8ACrrWf6vYgN2M4kS8gd4n7w/view?usp=sharing">
                  <span class="papertitle">Convolutional Color Constancy</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>ICCV</em>, 2015
                <br>
                <a href="https://drive.google.com/file/d/1vO3sVOMihmpNqsuASeR46Y_iME0lOANR/view?usp=sharing">supplement</a> / <a href="data/BarronICCV2015.bib">bibtex</a> / <a href="https://youtu.be/saHwKY9rfx0">video</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmalBNUzlENUJSVDg/view?usp=sharing">mp4</a>)
                <p></p>
                <p>By framing white balance as a chroma localization task we can discriminatively learn a color constancy model that beats the state-of-the-art by 40%.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/Shelhamer2015.jpg'>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1stygV71uBruD7Ck9CaAQr7nREvr3DtUL/view?usp=sharing">
                  <span class="papertitle">Scene Intrinsics and Depth from a Single Image</span>
                </a>
                <br>
                <a href="http://imaginarynumber.net/">Evan Shelhamer</a>, <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/%7Etrevor/">Trevor Darrell</a>
                <br>
                <em>ICCV Workshop</em>, 2015
                <br>
                <a href="data/Shelhamer2015.bib">bibtex</a>
                <p></p>
                <p>The monocular depth estimates produced by fully convolutional networks can be used to inform intrinsic image estimation.</p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0" onmouseout="defocus_stop()" onmouseover="defocus_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div id='lens_blurry' class='hidden'><img src="images/BarronCVPR2015_anim.gif"></div>
                <div id='lens_sharp'>
                  <a href="images/BarronCVPR2015_anim.gif"><img src="images/BarronCVPR2015_still.jpg"></a>
                </div>
                <script type="text/javascript">
                  function defocus_start() {
                    document.getElementById('lens_blurry').style.display = 'inline';
                    document.getElementById('lens_sharp').style.display = 'none';
                  }

                  function defocus_stop() {
                    document.getElementById('lens_blurry').style.display = 'none';
                    document.getElementById('lens_sharp').style.display = 'inline';
                  }
                  defocus_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1R4RdaBZIs-uJobhIFs9yKf3jIsaHQNH0/view?usp=sharing">
                  <span class="papertitle">Fast Bilateral-Space Stereo for Synthetic Defocus</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/abadams/">Andrew Adams</a>, <a href="http://people.csail.mit.edu/yichangshih/">YiChang Shih</a>, <a href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>
                <br>
                <em>CVPR</em>, 2015 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://drive.google.com/file/d/125qgMdqeT1vojMIijIKcOF099LjUgUOL/view?usp=sharing">abstract</a> /
                <a href="https://drive.google.com/file/d/1HGGvVOGxmPjvgdK5q3UD1Qb5Nttg6kq9/view?usp=sharing">supplement</a> /
                <a href="data/BarronCVPR2015.bib">bibtex</a> /
                <a href="http://techtalks.tv/talks/fast-bilateral-space-stereo-for-synthetic-defocus/61624/">talk</a> /
                <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmSzZZdUJSMllSUkE/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/open?id=0B4nuwEMaEsnmZ1ZXUzBCWDJYeFU">PDF</a>)
                <p></p>
                <p>By embedding a stereo optimization problem in "bilateral-space" we can very quickly solve for an edge-aware depth map, letting us render beautiful depth-of-field effects.</p>
                <p>This technology is used by the <a href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html">Google Camera "Lens Blur"</a> feature. </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/PABMM2015.jpg" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal">
                  <span class="papertitle">Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</span>
                </a>
                <br>
                <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>TPAMI</em>, 2017
                <br>
                <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
                <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
                <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a>
                <p></p>
                <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p>
                <p>This paper subsumes our CVPR 2014 paper.</p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0" onmouseout="sirfs_stop()" onmouseover="sirfs_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sirfs_image'>
                    <a href="images/Estee.png"><img src='images/Estee_160.png' style="border-style: none"></a>
                  </div>
                  <a href="images/Estee.png"><img src='images/Estee_160_prodB2.png' style="border-style: none"></a>
                </div>
                <script type="text/javascript">
                  function sirfs_start() {
                    document.getElementById('sirfs_image').style.opacity = "1";
                  }

                  function sirfs_stop() {
                    document.getElementById('sirfs_image').style.opacity = "0";
                  }
                  sirfs_stop()
                </script>
              </td>
              <td width="75%" valign="middle">
                <p>
                  <a href="https://arxiv.org/abs/2010.03592" id="SIRFS">
                    <span class="papertitle">Shape, Illumination, and Reflectance from Shading</span>
                  </a>
                  <br>
                  <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>TPAMI</em>, 2015
                  <br>
                  <a href="data/BarronMalikTPAMI2015.bib">bibtex</a> / <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmVWpfa19mbUxIYW8/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmazJvLXJUb0NuM1U/view?usp=sharing">powerpoint</a>, <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmTDBUWE96VHJndjg/view?usp=sharing">PDF</a>) / <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a> / <a href="https://drive.google.com/file/d/1vg9Rb-kBntSTnTCzVgFlskkPXvTB_5aq/view?usp=sharing">code &amp; data</a> / <a href="https://drive.google.com/file/d/11X5Zfjy7Q7oP_V2rtqy2f5-x9YgQUAFd/view?usp=sharing">kudos</a>
                </p>
                <p>
                  We present <strong>SIRFS</strong>, which can estimate shape, chromatic illumination, reflectance, and shading from a single image of an masked object.
                </p>
                <p>
                  This paper subsumes our CVPR 2011, CVPR 2012, and ECCV 2012 papers.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ArbalaezCVPR2014.jpg" alt="ArbalaezCVPR2014" width="160" height="120" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1M0wijHY134F9ETBgO8mjeuKUSblTRLG0/view?usp=sharing">
                  <span class="papertitle">Multiscale Combinatorial Grouping</span>
                </a>
                <br>
                <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>CVPR</em>, 2014
                <br>
                <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
                <a href="data/ArbelaezCVPR2014.bib">bibtex</a>
                <p>This paper is subsumed by <a href="#MCG_journal">our journal paper</a>.</p>
              </td>
            </tr>

            <tr onmouseout="flyspin_stop()" onmouseover="flyspin_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div id='flyspin' class='hidden'><img src="images/BarronICCV2013_160.gif"></div>
                <div id='flystill'>
                  <a href="images/BarronICCV2013.gif"><img src="images/BarronICCV2013_160.jpg"></a>
                </div>
                <script type="text/javascript">
                  function flyspin_start() {
                    document.getElementById('flyspin').style.display = 'inline';
                    document.getElementById('flystill').style.display = 'none';
                  }

                  function flyspin_stop() {
                    document.getElementById('flyspin').style.display = 'none';
                    document.getElementById('flystill').style.display = 'inline';
                  }
                  flyspin_stop()
                </script>
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1shvItvx_8Sb8QNXhrOXkuRmx2618iwNJ/view?usp=sharing">
                  <span class="papertitle">Volumetric Semantic Segmentation using Pyramid Context Features</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <a href="http://big.lbl.gov/">Soile V. E. Ker&aumlnen</a>, <a href="http://www.lbl.gov/gsd/biggin.html">Mark D. Biggin</a>,
                <br> <a href="http://dwknowles.lbl.gov/">David W. Knowles</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>ICCV</em>, 2013
                <br>
                <a href="https://drive.google.com/file/d/1htiLpMAcYLtuBthmAb4XHnOYxUbkfnqR/view?usp=sharing">supplement</a> /
                <a href="https://drive.google.com/file/d/1qoYeFNa443myn2SfcdhmCsYBqE9xQrPD/view?usp=sharing">poster</a> /
                <a href="data/BarronICCV2013.bib">bibtex</a> / <a href="http://www.youtube.com/watch?v=Y56-FcfnlVA&hd=1">video 1</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing">mp4</a>) / <a href="http://www.youtube.com/watch?v=mvRoYuP6-l4&hd=1">video 2</a> (or <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing">mp4</a>) / <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmSF9YdWJjQmh4QW8/view?usp=sharing">code &amp; data</a>
                <p>
                  We present a technique for efficient per-voxel linear classification, which enables accurate and fast semantic segmentation of volumetric Drosophila imagery.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/3DSP_160.jpg" alt="3DSP" width="160" height="120" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing" id="3DSP">
                  <span class="papertitle">3D Self-Portraits</span>
                </a>
                <br>
                <a href="http://www.hao-li.com/">Hao Li</a>, <a href="http://www.evouga.com/">Etienne Vouga</a>, Anton Gudym, <a href="http://www.cs.princeton.edu/~linjiel/">Linjie Luo</a>, <strong>Jonathan T. Barron</strong>, Gleb Gusev
                <br>
                <em>SIGGRAPH Asia</em>, 2013
                <br>
                <a href="http://www.youtube.com/watch?v=DmUkbZ0QMCA">video</a> / <a href="http://shapify.me/">shapify.me</a> / <a href="data/3DSP_siggraphAsia2013.bib">bibtex</a>
                <p>Our system allows users to create textured 3D models of themselves in arbitrary poses using only a single 3D sensor.</p>
              </td>
            </tr>

            <tr onmouseout="rgbd_stop()" onmouseover="rgbd_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div id='rgbd_anim' class='hidden'><img src="images/SceneSIRFS.gif"></div>
                <div id='rgbd_still'><img src="images/SceneSIRFS-still.jpg"></div>
                <script type="text/javascript">
                  function rgbd_start() {
                    document.getElementById('rgbd_anim').style.display = 'inline';
                    document.getElementById('rgbd_still').style.display = 'none';
                  }

                  function rgbd_stop() {
                    document.getElementById('rgbd_anim').style.display = 'none';
                    document.getElementById('rgbd_still').style.display = 'inline';
                  }
                  rgbd_stop()
                </script>
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1snypSLhzC0jXCchJRsWpcDZ7Es5hDmXo/view?usp=sharing">
                  <span class="papertitle">Intrinsic Scene Properties from a Single RGB-D Image</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>CVPR</em>, 2013 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://drive.google.com/file/d/1cLUw72WpgdZ_3TQAjJABdgywqjBfn_Mq/view?usp=sharing">supplement</a> / <a href="data/BarronMalikCVPR2013.bib">bibtex</a> / <a href="http://techtalks.tv/talks/intrinsic-scene-properties-from-a-single-rgb-d-image/58614/">talk</a> / <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmWW1CZGJPbi12R0k/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/file/d/19q3EFf6GIb4UFcCN2DVU2jVKpxRj5kxf/view?usp=sharing">powerpoint</a>, <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmMzQ4ZVp1SWdnVkk/view?usp=sharing">PDF</a>) / <a href="https://drive.google.com/open?id=1ZbPScVA6Efqd-ESvojl92sw8K-82Xxry">code &amp; data</a>
                <p>By embedding mixtures of shapes &amp; lights into a soft segmentation of an image, and by leveraging the output of the Kinect, we can extend SIRFS to scenes.
                  <br>
                  <br>TPAMI Journal version: <a href="https://drive.google.com/file/d/1iQiUxZvjPPnb8rFCwXYesTgFSRk7mkAq/view?usp=sharing">version</a> / <a href="data/BarronMalikTPAMI2015B.bib">bibtex</a>
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Boundary.jpg" alt="Boundary_png" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1H4YPovfrvcce3HGMEhidwU2l2fTcNR5y/view?usp=sharing">
                  <span class="papertitle">Boundary Cues for 3D Object Shape Recovery</span>
                </a>
                <br>
                <a href="http://www.kevinkarsch.com/">Kevin Karsch</a>,
                <a href="http://web.engr.illinois.edu/~liao17/">Zicheng Liao</a>,
                <a href="http://web.engr.illinois.edu/~jjrock2/">Jason Rock</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.illinois.edu/homes/dhoiem/">Derek Hoiem</a>
                <br>
                <em>CVPR</em>, 2013
                <br>
                <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmLUQ5SVJTcUZIYXc/view?usp=sharing">supplement</a> / <a href="data/KarschCVPR2013.bib">bibtex</a>
                <p>Boundary cues (like occlusions and folds) can be used for shape reconstruction, which improves object recognition for humans and computers.</p>
              </td>
            </tr>

            <tr onmouseout="eccv12_stop()" onmouseover="eccv12_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div id='eccv12_anim' class='hidden'>
                  <a href="https://drive.google.com/file/d/1brxb58CfRPe7KEER4Q_fYS9B_J-hiS0t/view?usp=sharing"><img src="images/ECCV2012_small.gif"></a>
                </div>
                <div id='eccv12_still'><img src="images/ECCV2012_still.jpg"></div>
                <script type="text/javascript">
                  function eccv12_start() {
                    document.getElementById('eccv12_anim').style.display = 'inline';
                    document.getElementById('eccv12_still').style.display = 'none';
                  }

                  function eccv12_stop() {
                    document.getElementById('eccv12_anim').style.display = 'none';
                    document.getElementById('eccv12_still').style.display = 'inline';
                  }
                  eccv12_stop()
                </script>
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1NczR4pJ-s0YBjCe0rCevMt8IM5JPuUrc/view?usp=sharing">
                  <span class="papertitle">Color Constancy, Intrinsic Images, and Shape Estimation</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>ECCV</em>, 2012
                <br>
                <a href="https://drive.google.com/file/d/1zuxhWZ3i6THvuRRBeE7dM_BJfDxO72Fq/view?usp=sharing">supplement</a> /
                <a href="data/BarronMalikECCV2012.bib">bibtex</a> /
                <a href="https://drive.google.com/file/d/12x8mhqpFsA6p0u6ZQW-ieRKF8hlQBKKe/view?usp=sharing">poster</a> /
                <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a>
                <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
              </td>
            </tr>

            <tr onmouseout="cvpr12_stop()" onmouseover="cvpr12_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" style="height: 120px">
                  <div class="two" id='cvpr12_image' style="height: 120px">
                    <img src='images/BarronCVPR2012_after.jpg' style="border-style: none">
                  </div>
                  <img src='images/BarronCVPR2012_before.jpg' style="border-style: none">
                </div>
                <script type="text/javascript">
                  function cvpr12_start() {
                    document.getElementById('cvpr12_image').style.opacity = "1";
                  }

                  function cvpr12_stop() {
                    document.getElementById('cvpr12_image').style.opacity = "0";
                  }
                  cvpr12_stop()
                </script>
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/17RfINbE2dr2EjXp9MtGO0MHJLQmQVhvT/view?usp=sharing">
                  <span class="papertitle">Shape, Albedo, and Illumination from a Single Image of an Unknown Object</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>CVPR</em>, 2012
                <br>
                <a href="https://drive.google.com/file/d/1Im_bUI42AP9VPoNtsjLajvtLRiwv39k3/view?usp=sharing">supplement</a> /
                <a href="data/BarronMalikCVPR2012.bib">bibtex</a> /
                <a href="https://drive.google.com/file/d/1IAlSF4k3_CEL9dfbaMiNTFPBoEkLhsRl/view?usp=sharing">poster</a>
                <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/B3DO.jpg" alt="b3do" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1_S8EQyngbHQrB415o0XkQ4V9SMzdEgWT/view?usp=sharing">
                  <span class="papertitle">A Category-Level 3-D Object Dataset: Putting the Kinect to Work</span>
                </a>
                <br>
                <a href="http://www.eecs.berkeley.edu/%7Eallie/">Allison Janoch</a>,
                <a href="http://sergeykarayev.com/">Sergey Karayev</a>,
                <a href="http://www.eecs.berkeley.edu/%7Ejiayq/">Yangqing Jia</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/%7Emfritz/">Mario Fritz</a>,
                <a href="http://www.icsi.berkeley.edu/%7Esaenko/">Kate Saenko</a>,
                <a href="http://www.eecs.berkeley.edu/%7Etrevor/">Trevor Darrell</a>
                <br>
                <em>ICCV 3DRR Workshop</em>, 2011
                <br>
                <a href="data/B3DO_ICCV_2011.bib">bibtex</a> /
                <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a>
                <p>We present a large RGB-D dataset of indoor scenes and investigate ways to improve object detection using depth information.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/safs.jpg" alt="safs_small" width="160" height="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1EZTOO5xezLYcyIFgAzs4KuZFLbTcwTDH/view?usp=sharing">
                  <span class="papertitle">High-Frequency Shape and Albedo from Shading using Natural Image Statistics</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>CVPR</em>, 2011
                <br>
                <a href="data/BarronMalikCVPR2011.bib">bibtex</a>
                <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/fast_texture.jpg" alt="fast-texture" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1rc05NatkQVmUDlGCAYcHSrvAzTpU9knT/view?usp=sharing">
                  <span class="papertitle">Discovering Efficiency in Coarse-To-Fine Texture Classification</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>Technical Report</em>, 2010
                <br>
                <a href="data/BarronTR2010.bib">bibtex</a>
                <p>A model and feature representation that allows for sub-linear coarse-to-fine semantic segmentation.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/prl.jpg" alt="prl" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
                  <span class="papertitle">Parallelizing Reinforcement Learning</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>
                <br>
                <em>Technical Report</em>, 2009
                <br>
                <a href="data/BarronPRL2009.bib">bibtex</a>
                <p>Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/bd_promo.jpg" alt="blind-date" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing">
                  <span class="papertitle">Blind Date: Using Proper Motions to Determine the Ages of Historical Images</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 136, 2008
                <p>Using the relative motions of stars we can accurately estimate the date of origin of historical astronomical images.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                  <span class="papertitle">Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 135, 2008
                <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
                <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
              </td>
            </tr>

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
